# DQN Algorithms Guide

LunarLander-v3ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  DQN ì•Œê³ ë¦¬ì¦˜ ë³€í˜• ê°€ì´ë“œ

---

## ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ì•Œê³ ë¦¬ì¦˜

### 1. Vanilla DQN
**ê¸°ë³¸ Deep Q-Network**

```bash
python train.py 1000 dqn
```

**íŠ¹ì§•:**
- ê°€ì¥ ê°„ë‹¨í•œ êµ¬í˜„
- Q(s,a) â† r + Î³ Ã— max_a' Q_target(s', a')
- ë¹ ë¥¸ ì´ˆê¸° í•™ìŠµ
- âš ï¸ Q-value ê³¼ëŒ€í‰ê°€ ë¬¸ì œ

**ê¶Œì¥ ì‚¬ìš©:**
- í•™ìŠµ ë° í”„ë¡œí† íƒ€ì…
- ì•Œê³ ë¦¬ì¦˜ ì´í•´ ëª©ì 

---

### 2. Double DQN (DDQN)
**Q-value ê³¼ëŒ€í‰ê°€ í•´ê²°**

```bash
python train.py 1000 ddqn
# ë˜ëŠ”
python train.py 1000 double_dqn
```

**íŠ¹ì§•:**
- í–‰ë™ ì„ íƒê³¼ í‰ê°€ ë¶„ë¦¬
- Q(s,a) â† r + Î³ Ã— Q_target(s', argmax_a' Q_online(s', a'))
- ì•ˆì •ì ì¸ í•™ìŠµ
- âœ… ì‹¤ì „ ë°°í¬ ê¶Œì¥

**ê¶Œì¥ ì‚¬ìš©:**
- ì‹¤ì „ ë°°í¬
- ì•ˆì •ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°

**ë…¼ë¬¸:** van Hasselt et al. (2016) "Deep RL with Double Q-learning"

---

### 3. Dueling DQN
**Valueì™€ Advantage ë¶„ë¦¬**

```bash
python train.py 1000 dueling
# ë˜ëŠ”
python train.py 1000 duel
python train.py 1000 dueling_dqn
```

**íŠ¹ì§•:**
- Q(s,a) = V(s) + (A(s,a) - mean(A(s,a')))
- Value stream: ìƒíƒœ ê°€ì¹˜ ì¶”ì •
- Advantage stream: í–‰ë™ì˜ ìƒëŒ€ì  ê°€ì¹˜
- ë” ë‚˜ì€ ìƒíƒœ ê°€ì¹˜ í•™ìŠµ

**ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°:**
```
Input (8)
    â†“
Feature Extraction (128 â†’ 128)
    â†“
    â”œâ”€â†’ Value Stream (64 â†’ 1)
    â””â”€â†’ Advantage Stream (64 â†’ 4)
         â†“
    Combine: V(s) + (A(s,a) - mean(A))
```

**ê¶Œì¥ ì‚¬ìš©:**
- ë§ì€ í–‰ë™ì´ ë¹„ìŠ·í•œ ê°€ì¹˜ë¥¼ ê°€ì§ˆ ë•Œ
- ìƒíƒœ ê°€ì¹˜ ì¶”ì •ì´ ì¤‘ìš”í•œ ê²½ìš°

**ë…¼ë¬¸:** Wang et al. (2016) "Dueling Network Architectures for Deep RL"

---

### 4. Dueling Double DQN (D3QN) â­ ìµœê³ 
**ë‘ ê¸°ë²•ì˜ ê²°í•©**

```bash
python train.py 1000 d3qn
# ë˜ëŠ”
python train.py 1000 dueling_ddqn
python train.py 1000 dueling_double_dqn
```

**íŠ¹ì§•:**
- Dueling ì•„í‚¤í…ì²˜ + Double DQN í•™ìŠµ
- Q-value ê³¼ëŒ€í‰ê°€ ë°©ì§€ âœ…
- ë” ë‚˜ì€ ìƒíƒœ ê°€ì¹˜ ì¶”ì • âœ…
- ê°€ì¥ ì•ˆì •ì ì´ê³  ì„±ëŠ¥ ì¢‹ìŒ âœ…

**ê¶Œì¥ ì‚¬ìš©:**
- ìµœê³  ì„±ëŠ¥ì„ ì›í•  ë•Œ (ê°•ë ¥ ê¶Œì¥!)
- ì¥ê¸° í•™ìŠµì´ ê°€ëŠ¥í•œ ê²½ìš°
- ì‹¤ì „ ë°°í¬ (ìµœì )

**ë…¼ë¬¸:**
- Wang et al. (2016) "Dueling Network Architectures"
- van Hasselt et al. (2016) "Double Q-learning"

---

## ğŸ“Š ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ìš”ì•½

| ì•Œê³ ë¦¬ì¦˜ | Q-value ì •í™•ë„ | í•™ìŠµ ì•ˆì •ì„± | ìˆ˜ë ´ ì†ë„ | ë³µì¡ë„ | ì¶”ì²œë„ |
|---------|--------------|-----------|----------|--------|--------|
| Vanilla DQN | â­â­ | â­â­â­ | â­â­â­â­ | â­ | â­â­â­ |
| Double DQN | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­â­â­ |
| Dueling DQN | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­ |
| D3QN | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |

---

## ğŸ”¬ í•µì‹¬ ì°¨ì´ì 

### Q-value ê³„ì‚° ë°©ì‹

#### Vanilla DQN & Double DQN
```python
# ë‹¨ì¼ ì¶œë ¥ ìŠ¤íŠ¸ë¦¼
Q(s) = [Q(s,aâ‚€), Q(s,aâ‚), Q(s,aâ‚‚), Q(s,aâ‚ƒ)]
```

#### Dueling DQN & D3QN
```python
# ë‘ ê°œì˜ ìŠ¤íŠ¸ë¦¼ ë¶„ë¦¬
V(s) = ìƒíƒœ ê°€ì¹˜ (ìŠ¤ì¹¼ë¼)
A(s,a) = [A(s,aâ‚€), A(s,aâ‚), A(s,aâ‚‚), A(s,aâ‚ƒ)]

# ê²°í•©
Q(s,a) = V(s) + (A(s,a) - mean(A(s,a')))
```

### Target Q-value ê³„ì‚°

#### Vanilla DQN & Dueling DQN
```python
# Max ì—°ì‚°ì ì‚¬ìš©
target_q = r + Î³ Ã— max_a' Q_target(s', a')
```

#### Double DQN & D3QN
```python
# í–‰ë™ ì„ íƒê³¼ í‰ê°€ ë¶„ë¦¬
a_best = argmax_a' Q_online(s', a')  # ì˜¨ë¼ì¸ìœ¼ë¡œ ì„ íƒ
target_q = r + Î³ Ã— Q_target(s', a_best)  # íƒ€ê²Ÿìœ¼ë¡œ í‰ê°€
```

---

## ğŸ’¡ ì‹¤ì „ ì‚¬ìš© ê°€ì´ë“œ

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…
**ì„ íƒ:** Vanilla DQN
```bash
python train.py 500 dqn
```
- ë¹ ë¥¸ ê°œë°œ
- ê°„ë‹¨í•œ êµ¬í˜„
- ì´ˆê¸° ê²°ê³¼ í™•ì¸

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì‹¤ì „ ë°°í¬ (ì•ˆì •ì„± ì¤‘ìš”)
**ì„ íƒ:** Double DQN
```bash
python train.py 1000 ddqn
```
- ì•ˆì •ì ì¸ í•™ìŠµ
- ì¼ê´€ëœ ì„±ëŠ¥
- ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜

### ì‹œë‚˜ë¦¬ì˜¤ 3: ìµœê³  ì„±ëŠ¥ (ì‹œê°„ ì¶©ë¶„)
**ì„ íƒ:** D3QN â­
```bash
python train.py 1000 d3qn
```
- ê°€ì¥ ì•ˆì •ì 
- ìµœê³  ì„±ëŠ¥
- ì¥ê¸° í•™ìŠµ ê°€ëŠ¥

### ì‹œë‚˜ë¦¬ì˜¤ 4: ì—°êµ¬ / ë¹„êµ
**ì„ íƒ:** ì „ì²´ ë¹„êµ
```bash
# ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
python train.py 1000 dqn
python train.py 1000 ddqn
python train.py 1000 dueling
python train.py 1000 d3qn
```

---

## ğŸ“ í•™ìŠµ ì˜ˆì‹œ

### ê¸°ë³¸ í•™ìŠµ
```bash
# Double DQNìœ¼ë¡œ 500 ì—í”¼ì†Œë“œ (ê¶Œì¥)
python train.py 500 ddqn

# D3QNìœ¼ë¡œ 1000 ì—í”¼ì†Œë“œ (ìµœê³ )
python train.py 1000 d3qn
```

### GUI í‘œì‹œí•˜ë©° í•™ìŠµ
```bash
# í•™ìŠµ ê³¼ì • ì‹¤ì‹œê°„ ê´€ì°°
python train.py 500 ddqn --show-gui
python train.py 1000 d3qn --show-gui
```

### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
```bash
# 60 ì—í”¼ì†Œë“œë¡œ ë¹ ë¥¸ í™•ì¸
python train.py 60 dqn
python train.py 60 ddqn
python train.py 60 dueling
python train.py 60 d3qn
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥

### 1000 ì—í”¼ì†Œë“œ í•™ìŠµ í›„

| ì•Œê³ ë¦¬ì¦˜ | ì˜ˆìƒ ìµœê³  ë³´ìƒ | ì˜ˆìƒ ì„±ê³µë¥  | í•™ìŠµ ì‹œê°„ |
|---------|--------------|-----------|----------|
| Vanilla DQN | 280-310 | 60-70% | ~7ë¶„ |
| Double DQN | 300-315 | 70-80% | ~7ë¶„ |
| Dueling DQN | 290-320 | 65-75% | ~8ë¶„ |
| D3QN | **310-330** | **75-85%** | ~8ë¶„ |

*Apple Silicon ê¸°ì¤€

---

## ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„°

ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì— ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©:

```python
# ë„¤íŠ¸ì›Œí¬
hidden_dim = 128          # Vanilla/Double: [8,128,128,4]
                         # Dueling/D3QN: [8,128,128] â†’ V(64,1) + A(64,4)

# í•™ìŠµ
learning_rate = 1e-3
gamma = 0.99
batch_size = 64
buffer_capacity = 10000

# íƒìƒ‰
epsilon_start = 1.0
epsilon_end = 0.01
epsilon_decay = 0.995

# ì—…ë°ì´íŠ¸
target_update_freq = 10  # episodes
```

---

## ğŸ“š ì°¸ê³  ë…¼ë¬¸

### DQN (2015)
- **ì œëª©:** Playing Atari with Deep Reinforcement Learning
- **ì €ì:** Mnih et al.
- **ë§í¬:** https://arxiv.org/abs/1312.5602

### Double DQN (2016)
- **ì œëª©:** Deep Reinforcement Learning with Double Q-learning
- **ì €ì:** van Hasselt, Guez, Silver
- **ë§í¬:** https://arxiv.org/abs/1509.06461

### Dueling DQN (2016)
- **ì œëª©:** Dueling Network Architectures for Deep Reinforcement Learning
- **ì €ì:** Wang et al.
- **ë§í¬:** https://arxiv.org/abs/1511.06581

---

## ğŸ¯ ê¶Œì¥ í•™ìŠµ ìˆœì„œ

### ì´ˆë³´ì
1. **Vanilla DQN** (500 ì—í”¼ì†Œë“œ) - ê¸°ë³¸ ì´í•´
2. **Double DQN** (1000 ì—í”¼ì†Œë“œ) - ì•ˆì •ì„± ì²´ê°
3. **ê²°ê³¼ ë¹„êµ** - ì°¨ì´ì  ë¶„ì„

### ì¤‘ê¸‰ì
1. **Double DQN** (1000 ì—í”¼ì†Œë“œ) - ë² ì´ìŠ¤ë¼ì¸
2. **Dueling DQN** (1000 ì—í”¼ì†Œë“œ) - ì•„í‚¤í…ì²˜ ë¹„êµ
3. **D3QN** (1000 ì—í”¼ì†Œë“œ) - ìµœì¢… ì„±ëŠ¥

### ê³ ê¸‰ì / ì—°êµ¬
1. ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ **2000 ì—í”¼ì†Œë“œ** í•™ìŠµ
2. ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜
3. í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ (ì—¬ëŸ¬ seed)

---

## ğŸ’» ì½”ë“œ ì˜ˆì‹œ

### Python ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì§ì ‘ ì‚¬ìš©

```python
from train import DQNAgent, DoubleDQNAgent, DuelingDQNAgent, DuelingDoubleDQNAgent
import gymnasium as gym

# í™˜ê²½ ìƒì„±
env = gym.make("LunarLander-v3")
state_dim = 8
action_dim = 4

# 1. Vanilla DQN
agent = DQNAgent(state_dim, action_dim)

# 2. Double DQN
agent = DoubleDQNAgent(state_dim, action_dim)

# 3. Dueling DQN
agent = DuelingDQNAgent(state_dim, action_dim)

# 4. Dueling Double DQN (D3QN)
agent = DuelingDoubleDQNAgent(state_dim, action_dim)

# í•™ìŠµ ë£¨í”„
for episode in range(1000):
    obs, _ = env.reset()
    done = False

    while not done:
        action = agent.select_action(obs)
        next_obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        agent.replay_buffer.push(obs, action, reward, next_obs, done)
        agent.train_step()

        obs = next_obs

    # ì£¼ê¸°ì ìœ¼ë¡œ target network ì—…ë°ì´íŠ¸
    if episode % 10 == 0:
        agent.update_target_network()
```

---

## ğŸ† ìµœì¢… ê¶Œì¥ ì‚¬í•­

### ì¼ë°˜ ì‚¬ìš©ì
**ì„ íƒ:** Double DQN (ddqn)
- ì•ˆì •ì ì´ê³  ê²€ì¦ë¨
- ë¹ ë¥¸ ìˆ˜ë ´
- ì¢‹ì€ ì„±ëŠ¥

```bash
python train.py 1000 ddqn
```

### ìµœê³  ì„±ëŠ¥ ì¶”êµ¬
**ì„ íƒ:** D3QN
- ê°€ì¥ ì•ˆì •ì 
- ìµœê³  ì„±ëŠ¥
- ìƒíƒœ ê°€ì¹˜ í•™ìŠµ ìš°ìˆ˜

```bash
python train.py 1000 d3qn
```

### í•™ìŠµ / ì—°êµ¬
**ì„ íƒ:** ëª¨ë‘ ë¹„êµ
- ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„± ì´í•´
- ì„±ëŠ¥ ì°¨ì´ ë¶„ì„
- ë…¼ë¬¸ ì‘ì„±ìš©

```bash
for algo in dqn ddqn dueling d3qn; do
    python train.py 1000 $algo
done
```

---

**Updated:** 2025-11-21
**Available Algorithms:** 4 (DQN, Double DQN, Dueling DQN, D3QN)
**Recommendation:** D3QN for best results, DDQN for stability ğŸš€
